{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CCT_Tokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Tokenizer for CCT.\n",
    "    Replaces the standard patching with a convolutional block to preserve local spatial relationships[cite: 317].\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, embed_dim=256, kernel_size=7, stride=2, padding=3):\n",
    "        super(CCT_Tokenizer, self).__init__()\n",
    "        # Standard initial convolution to reduce dimensions and tokenize\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(64, embed_dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # Flatten spatial dimensions to create sequence of tokens: (B, C, H, W) -> (B, H*W, C)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class SequencePooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence Pooling (SeqPool) as described in CCT papers and utilized here.\n",
    "    Gathers sequential information without a CLS token[cite: 276, 277].\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.attention_pool = nn.Linear(embed_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (Batch, Seq_Len, Embed_Dim)\n",
    "        # Calculate weights for each token\n",
    "        w = self.attention_pool(x) # (Batch, Seq_Len, 1)\n",
    "        w = self.softmax(w)\n",
    "        # Weighted sum of tokens\n",
    "        out = torch.matmul(x.transpose(1, 2), w).squeeze(-1) # (Batch, Embed_Dim)\n",
    "        return out\n",
    "\n",
    "class CompactConvolutionalTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, embed_dim=128, num_layers=4, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.tokenizer = CCT_Tokenizer(img_size=img_size, embed_dim=embed_dim)\n",
    "\n",
    "        # Positional Embedding (Learnable)\n",
    "        # Note: Sequence length depends on input size and tokenizer strides.\n",
    "        # For 224x224 and standard strides, seq_len is approx 56*56 or 28*28 depending on depth.\n",
    "        # We allow a max length or dynamic handling.\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 3136, embed_dim)) # Approx max seq len\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.seq_pool = SequencePooling(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        B, N, C = x.shape\n",
    "        # Add positional embeddings (truncated to current sequence length)\n",
    "        x = x + self.pos_embed[:, :N, :]\n",
    "        x = self.transformer(x)\n",
    "        x = self.seq_pool(x)\n",
    "        return x\n",
    "\n",
    "class HybridEarNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The proposed hybrid architecture: MobileNetV2 + ViT (CCT)[cite: 23, 319].\n",
    "    \"\"\"\n",
    "    def __init__(self, cct_embed_dim=128):\n",
    "        super(HybridEarNet, self).__init__()\n",
    "\n",
    "        # --- Branch 1: MobileNetV2 ---\n",
    "        # \"we used the CNN model MobileNetV2\" [cite: 315]\n",
    "        # \"the last layer of MobileNetV2 is removed and its feature vector of 1280 is concatenated\" [cite: 319]\n",
    "        base_mobilenet = models.mobilenet_v2(pretrained=True)\n",
    "        self.mobilenet_features = base_mobilenet.features\n",
    "        self.mobilenet_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # --- Branch 2: CCT (Vision Transformer) ---\n",
    "        # \"CCT architecture... reduces the number of parameters\" [cite: 317]\n",
    "        self.cct = CompactConvolutionalTransformer(embed_dim=cct_embed_dim)\n",
    "\n",
    "        # --- Fusion Head ---\n",
    "        # MobileNetV2 outputs 1280 dim. CCT outputs cct_embed_dim.\n",
    "        # Figure 4 shows these concatenated, then into a FC layer of 512, then to 2 classes.\n",
    "        combined_dim = 1280 + cct_embed_dim\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512), # \"FC 512\" from Figure 4 [cite: 302]\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2), # Standard practice, though not explicitly detailed in text\n",
    "            nn.Linear(512, 2) # \"Male / Female\" [cite: 308]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. MobileNet Path\n",
    "        m_out = self.mobilenet_features(x)\n",
    "        m_out = self.mobilenet_pool(m_out)\n",
    "        m_out = torch.flatten(m_out, 1) # (Batch, 1280)\n",
    "\n",
    "        # 2. CCT Path\n",
    "        c_out = self.cct(x) # (Batch, cct_embed_dim)\n",
    "\n",
    "        # 3. Concatenation\n",
    "        # \"concatenated with the feature vector of CCT model\" [cite: 319]\n",
    "        combined = torch.cat((m_out, c_out), dim=1)\n",
    "\n",
    "        # 4. Classification\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "\n",
    "# Instantiate the model\n",
    "model = HybridEarNet(cct_embed_dim=128)\n",
    "print(f\"Model created. Parameters should be efficient (Low parameter count target).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
