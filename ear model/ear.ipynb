{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EarDatasetStructured(Dataset):\n",
    "    def __init__(self, root_dir=\"/Images\", img_size=128, samples_per_folder=10):\n",
    "        \"\"\"\n",
    "        root_dir: 'ear model/Images'\n",
    "        Folder names look like: '001.ALI_HD', '002.LeDuong_BL', etc.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = []\n",
    "        self.img_size = img_size\n",
    "        self.samples_per_folder = samples_per_folder\n",
    "\n",
    "        # list folders sorted: 001..., 002..., ..., 164...\n",
    "        folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for folder in folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if not os.path.isdir(folder_path):\n",
    "                continue\n",
    "\n",
    "            # extract numeric ID from folder name\n",
    "            # example: \"001.ALI_HD\" → 1\n",
    "            folder_num_str = folder.split(\".\")[0]  # '001'\n",
    "            folder_num = int(folder_num_str)\n",
    "\n",
    "            # label assignment\n",
    "            if 1 <= folder_num <= 98:\n",
    "                label = 0  # male\n",
    "            else:\n",
    "                label = 1  # female\n",
    "\n",
    "            # collect all image paths\n",
    "            img_files = [\n",
    "                os.path.join(folder_path, f)\n",
    "                for f in os.listdir(folder_path)\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "            ]\n",
    "\n",
    "            # sample a fixed number\n",
    "            if len(img_files) > samples_per_folder:\n",
    "                img_files = random.sample(img_files, samples_per_folder)\n",
    "\n",
    "            for img_path in img_files:\n",
    "                self.data.append((img_path, label))\n",
    "\n",
    "        print(f\"Loaded {len(self.data)} images from {len(folders)} folders\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "\n",
    "        # Load image with OpenCV\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        # HWC → CHW\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        img = torch.tensor(img)\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return img, label\n"
   ],
   "id": "7149b1d13f2d195c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64×64\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32×32\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16×16\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ],
   "id": "8d1cdb07c7d7c362",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for imgs, labels in tqdm(loader):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_acc += (preds == labels).float().mean().item()\n",
    "\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n"
   ],
   "id": "dcf06cdcdfc971e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_acc += (preds == labels).float().mean().item()\n",
    "\n",
    "    return total_loss / len(loader), total_acc / len(loader)\n"
   ],
   "id": "d221b37c46c0cff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "dataset = EarDatasetStructured(\n",
    "    root_dir=\"Images\",\n",
    "    img_size=128,\n",
    "    samples_per_folder=20\n",
    ")\n",
    "\n",
    "# Train/Val split\n",
    "idx_train, idx_val = train_test_split(\n",
    "    np.arange(len(dataset)), test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "train_subset = torch.utils.data.Subset(dataset, idx_train)\n",
    "val_subset = torch.utils.data.Subset(dataset, idx_val)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = SimpleCNN(num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train\n",
    "for epoch in range(14):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Acc={train_acc:.3f} | Val Acc={val_acc:.3f}\")\n"
   ],
   "id": "29adeddc9f3d979e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_one_image_per_folder(root_dir=\"Images\"):\n",
    "    folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "    plt.figure(figsize=(12, 20))\n",
    "\n",
    "    i = 1\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # find first image\n",
    "        imgs = [f for f in os.listdir(folder_path) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
    "        if len(imgs) == 0:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder_path, imgs[0])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.subplot(len(folders)//5 + 1, 5, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(folder)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        i += 1\n",
    "        if i > 50:  # show first 50 for convenience\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call it:\n",
    "show_one_image_per_folder(\"Images\")\n"
   ],
   "id": "4c25f47fab686213",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def predict_gender(model, image_path, img_size=128):\n",
    "    # Load with OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocess (same as training)\n",
    "    img_resized = cv2.resize(img_rgb, (img_size, img_size))\n",
    "    img_norm = img_resized.astype(np.float32) / 255.0\n",
    "    img_chw = np.transpose(img_norm, (2, 0, 1))  # HWC → CHW\n",
    "\n",
    "    # Create torch tensor\n",
    "    input_tensor = torch.tensor(img_chw).unsqueeze(0).to(device)  # (1,3,H,W)\n",
    "\n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    # Convert label to text\n",
    "    label = \"Male\" if pred == 0 else \"Female\"\n",
    "\n",
    "    # Show image + label\n",
    "    cv2.putText(img_rgb, label, (20, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 0), 3)\n",
    "\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Prediction: {label}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    return label\n"
   ],
   "id": "b95b2f033bfb325c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), \"ear_model.pth\")\n",
   "id": "dfeb38ca48f62298",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "for i in range(8):\n",
    "    test_image = f\"img_{i}.png\"\n",
    "    if not os.path.exists(test_image):\n",
    "        print(f\"File not found: {test_image}\")\n",
    "    else:\n",
    "        result = predict_gender(model, test_image)\n",
    "        print(f\"{test_image} -> {result}\")\n"
   ],
   "id": "91860ab0d87d89ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "test_image = \"img.png\"\n",
    "predict_gender(model, test_image)\n",
    "\n",
    "test_image2 = \"img_1.png\"\n",
    "predict_gender(model, test_image2)\n",
    "\n",
    "\n",
    "test_image3 = \"img_2.png\"\n",
    "predict_gender(model, test_image3)\n",
    "\n",
    "\n",
    "def test_one_from_each_folder(model, root_dir=\"Images\"):\n",
    "    folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        imgs = [f for f in os.listdir(folder_path) if f.lower().endswith((\".jpg\",\".png\",\".jpeg\"))]\n",
    "        if len(imgs) == 0:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder_path, imgs[0])\n",
    "\n",
    "        print(f\"{folder} → {predict_gender(model, img_path)}\")\n",
    "\n",
    "# Call it:\n",
    "test_one_from_each_folder(model, \"Images\")\n"
   ],
   "id": "fc80db2db9a9454f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Assuming these lists are filled during training:\n",
    "# train_losses, val_losses, train_accs, val_accs\n",
    "# And validation outputs: all_labels, all_preds\n",
    "\n",
    "# --------- 1. Plot Loss & Accuracy ---------\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(val_accs, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy Curve\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------- 2. Confusion Matrix ---------\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Male\",\"Female\"], yticklabels=[\"Male\",\"Female\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "35cd4be34028c9ed",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-gpu",
   "language": "python",
   "display_name": "TensorFlow GPU"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
